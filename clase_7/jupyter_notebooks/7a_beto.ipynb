{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7a - beto.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G63Jpt-wYcJ3"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Beto\n",
        "[GitHub LINK](https://github.com/dccuchile/beto)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcPiEBdt8NqM"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvoZ8YlK0vOQ"
      },
      "source": [
        "## 1 - BETO for Masked predictions\n",
        "Se necesita instalar la librería de \"transformers\" para utilizar los modelos de BERT y sus funciones de ayuda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vQGeFm0pdn1"
      },
      "source": [
        "!pip install transformers --quiet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJHhebCnnvnn"
      },
      "source": [
        "from transformers import TFBertForMaskedLM, BertTokenizer\n",
        "\n",
        "# Muy importante que para tensorflow los modelos Bert deben empezar con \"TF\"\n",
        "# de lo contrario estaremos utilizando un modelo para pytorch\n",
        "\n",
        "# Descargamos el modelo base de BETO y su correspondiente tokenizer (BERT para español)\n",
        "model = TFBertForMaskedLM.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmg5ZuNb1Bh6"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1fBV3ElzDZf"
      },
      "source": [
        "text = \"[CLS] Para solucionar los [MASK] del país, el presidente debe [MASK] de inmediato. [SEP]\"\n",
        "masked_indxs = (4,11)\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8B2alArhzKVb"
      },
      "source": [
        "# Sentencias transformadas en tokens\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwaq1y8gzMTe"
      },
      "source": [
        "# Tokens transformados a Ids\n",
        "print(indexed_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCggZIN8c_OH"
      },
      "source": [
        "tokens_tensor = tokenizer(text, return_tensors='tf')\n",
        "tokens_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt0T451xIdfI"
      },
      "source": [
        "predictions = model(tokens_tensor)[0]\n",
        "predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADwOxLevzyaS"
      },
      "source": [
        "print(\"Texto de entrada:\", text)\n",
        "\n",
        "predictions = model(tokens_tensor)[0]\n",
        "for i,midx in enumerate(masked_indxs):\n",
        "    # El sistema predice los posibles ids de palabras por cada MASK (en este caso 2)\n",
        "    # Se toman los 5 IDs con mayor probabilidad    \n",
        "    idxs = tf.argsort(predictions[0,midx], direction='DESCENDING')\n",
        "    # Esos IDs se transformar en palabras/tokens usando el tokenizar\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens(idxs[:5])\n",
        "    print('MASK',i,':',predicted_token)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}