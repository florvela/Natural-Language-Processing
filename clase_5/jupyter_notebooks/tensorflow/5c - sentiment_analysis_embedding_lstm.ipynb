{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"5c - sentiment_analysis_embedding_lstm.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPos0m1t5CA48B4ol2RVa7G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kBLpTr7plguX"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Sentiment analysis con Embeddings + LSTM"]},{"cell_type":"markdown","metadata":{"id":"9W6nuajhlqZD"},"source":["### Objetivo\n","El objetivo es utilizar las críticas de películas para que el sistema determine si la evaluación es positiva o negativa (sentiment analysis como clasificador binario de texto)"]},{"cell_type":"code","source":["!pip install --upgrade --no-cache-dir gdown --quiet"],"metadata":{"id":"6F9d3HetZ0Pc"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hCpOVzJdl8_p"},"source":["import numpy as np\n","import random\n","import io\n","import pickle\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","from keras.utils.np_utils import to_categorical\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oDbSydDfza5u"},"source":["from sklearn.metrics import roc_curve\n","from sklearn.metrics import auc\n","\n","def curva_roc(model, X_test, y_test):\n","    y_hat_prob = model.predict(X_test).ravel()\n","    #y_hat = [1 if x >= 0.5 else 0 for x in y_hat_prob]\n","    mask_positive = y_hat_prob >= 0.5\n","    mask_negative = y_hat_prob < 0.5\n","    y_hat = y_hat_prob\n","    y_hat[mask_positive] = 1\n","    y_hat[mask_negative] = 0\n","    y_hat = y_hat.astype(int)\n","\n","    # Calcular la exactitud (accuracy)\n","    scores = model.evaluate(X_test, y_test)\n","    print(\"Accuracy:\", scores[1])\n","\n","    fpr, tpr, thresholds = roc_curve(y_test, y_hat_prob)\n","    auc_keras = auc(fpr, tpr)\n","    print('auc_keras', auc_keras)\n","\n","    plt.figure(1)\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.plot(fpr, tpr, label='Keras (area = {:.3f})'.format(auc_keras))\n","    plt.xlabel('False positive rate')\n","    plt.ylabel('True positive rate')\n","    plt.title('Curva ROC test')\n","    plt.legend(loc='best')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8UPeRkrAmbF3"},"source":["### Datos\n","Utilizaremos como dataset críticas de películas de IMDB puntuadas deforma positiva o negativa.\\\n","Referencia del dataset: [LINK](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"]},{"cell_type":"code","metadata":{"id":"C7jLvTU3lSyL"},"source":["# Descargar la carpeta de dataset\n","import os\n","import gdown\n","if os.access('imdb_dataset.csv', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1fXW-u9NVbH1yhwU1AHzPVtgGyV1c8N3g'\n","    output = 'imdb_dataset.csv'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"El dataset ya se encuentra descargado\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o-SV1P3dnD1J"},"source":["# Armar el dataset\n","df = pd.read_csv('imdb_dataset.csv')\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q-OwSePKm-FK"},"source":["### 1 - Limpieza de datos\n","- En los datos se observo que en la columna \"review\" hay código HTML de salto de línea.\n","- Tranformar la columna snetiment a 0 y 1\n","\n"]},{"cell_type":"code","metadata":{"id":"-hc7-AmYnPC3"},"source":["# En los datos se observó código de HTML de salto de línea <br />\n","import re\n","df_reviews = df.copy() \n","df_reviews['review'] = df['review'].apply(lambda x: re.sub(\"<br />\", \"\", x))\n","df_reviews['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n","df_reviews.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sZtvASVOn3ty"},"source":["# Observar como está distribuido el dataset respecto a la columna Rating\n","# es decir, observar que tan balanceado se encuentra respecot a cada clase\n","df_reviews['sentiment'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v7QJ2poZn9b-"},"source":["# Observar como está distribuido el dataset\n","sns.countplot(x='sentiment', data=df_reviews)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"juVSYR89x_2v"},"source":["Se puede observar que el dataset está perfectamente balanceado"]},{"cell_type":"code","metadata":{"id":"gVJ_RVi4o1h3"},"source":["# Tomar la columna de las review y almacenarlo todo en un vector numpy de reviews\n","text_sequences = df_reviews['review'].values\n","text_sequences.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4nT5Un_co65Q"},"source":["# Cuantas reviews (rows) hay para evaluar?\n","len(text_sequences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HP5uN9tqpHu_"},"source":["# Concatenar todas las reviews para armar el corpus\n","corpus = ' '.join(text_sequences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FEzmePgdpf74"},"source":["# ¿Cuál es la longitud de ese corpus?\n","len(corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MYeJLdDmpvOe"},"source":["# Utilizar \"text_to_word_sequence\" para separar las palabras en tokens\n","# recordar que text_to_word_sequence automaticamente quita los signos de puntuacion y pasa el texto a lowercase\n","from keras.preprocessing.text import text_to_word_sequence\n","tokens = text_to_word_sequence(corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M6L-fnWAp_lA"},"source":["# Dar un vistazo a los primeros 20 tokens/palabras\n","tokens[:20]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-8QgwwMUqG0d"},"source":["# ¿Cuántos tokens/palabras hay?\n","len(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TFukNZdOsZ8_"},"source":["# Tokenizar las palabras con el Tokenizer de Keras\n","# Definir una máxima cantidad de palabras a utilizar:\n","# - num_words --> the maximum number of words to keep, based on word frequency.\n","# - Only the most common num_words-1 words will be kept.\n","from keras.preprocessing.text import Tokenizer\n","num_words = 2000\n","vocab_size = num_words\n","tok = Tokenizer(num_words=2000) \n","tok.fit_on_texts(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JnR1tlqZy94X"},"source":["# Obtener el diccionario de palabra (word) a índice\n","# y observar la cantidad total del vocabulario\n","word_index = tok.word_index\n","len(word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AvWzzSretQXf"},"source":["# Convertir las palabras/tokens a números\n","sequences = tok.texts_to_sequences(text_sequences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hfedPDOQD64v"},"source":["sequences[0][:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"za73M5SRtbrP"},"source":["# Determinar cual es la oración más larga\n","max(len(s) for s in sequences)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCkO9Wc9tls1"},"source":["# Realizar padding de las sentencias al mismo tamaño\n","# tomando de referencia la máxima sentencia\n","from keras.preprocessing.sequence import pad_sequences\n","maxlen = 200\n","\n","# Al realizar padding obtener la variable \"X\" (input)\n","X = pad_sequences(sequences, padding='pre', maxlen=maxlen)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kGHHabVdt_aa"},"source":["# Observar las dimensiones de la variable input\n","X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"llVM-tzQo9_F"},"source":["# Tomar la columna rating y alcemacenarla en una variable \"y\"\n","# Su shape debe ser equivalente la cantidad de rows del corpus\n","y = df_reviews['sentiment'].values\n","print(y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rmz9A6n4uK4V"},"source":["# Dividir los datos en train y test\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EcDPlhEouQ9E"},"source":["# Determinar la dimensiones de entrada y salida\n","in_shape = X_train.shape[1] # max input sentence len\n","out_shape = 1 # binary classification\n","print(\"in_shape\", in_shape, \", out_shape\", out_shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NpbQHExL6OTu"},"source":["### 2 - Entrenar el modelo con Embeddings + LSTM"]},{"cell_type":"code","metadata":{"id":"NUkuWBsM6cx3"},"source":["# Entrenar un modelo con LSTM entrenando sus propios embeddings\n","# o utilizando embeddings pre-entrenados.\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","from keras.layers import Dropout\n","\n","model = Sequential()\n","# input_dim = vocab_size (max n_words)\n","# input_length = setencias con padding a 200\n","# output_dim = 50 --> crear embeddings de tamaño 50\n","model.add(Embedding(input_dim=vocab_size+1, output_dim=50, input_length=in_shape))\n","model.add(LSTM(units=64, return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(units=64)) # La última capa LSTM no lleva return_sequences\n","\n","model.add(Dense(units=128, activation='relu'))\n","model.add(Dropout(rate=0.2))\n","model.add(Dense(units=out_shape, activation='sigmoid'))\n","\n","model.compile(optimizer=\"adam\",\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IiHo33J8opab"},"source":["from keras.utils.vis_utils import plot_model\n","plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_oTSAIjeo73-"},"source":["hist = model.fit(X_train, y_train, epochs=5, validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y-cjIatVpPqW"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","epoch_count = range(1, len(hist.history['accuracy']) + 1)\n","sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n","sns.lineplot(x=epoch_count,  y=hist.history['val_accuracy'], label='valid')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jx1tLx23pbRi"},"source":["model.evaluate(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yr78NmybzPMP"},"source":["# Como este modelo es binario podemos calcular la curva ROC\n","curva_roc(model, X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vkkl3FTL6Uhk"},"source":["### 3 - Entrenar el modelo con Embeddings Fasttext + LSTM"]},{"cell_type":"code","metadata":{"id":"tIgZkwiNprmG"},"source":["# Descargar los embeddings desde un gogle drive (es la forma más rápida)\n","# NOTA: No hay garantía de que estos links perduren, en caso de que no estén\n","# disponibles descargar de la página oficial como se explica en el siguiente bloque\n","import os\n","import gdown\n","if os.access('fasttext.pkl', os.F_OK) is False:\n","    url = 'https://drive.google.com/uc?id=1KU5qmAYh3LATMvVgocFDfW-PK3prm1WU&export=download'\n","    output = 'fasttext.pkl'\n","    gdown.download(url, output, quiet=False)\n","else:\n","    print(\"Los embeddings fasttext.pkl ya están descargados\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QMbkn_KppuDI"},"source":["import logging\n","import os\n","from pathlib import Path\n","from io import StringIO\n","import pickle\n","\n","class WordsEmbeddings(object):\n","    logger = logging.getLogger(__name__)\n","\n","    def __init__(self):\n","        # load the embeddings\n","        words_embedding_pkl = Path(self.PKL_PATH)\n","        if not words_embedding_pkl.is_file():\n","            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n","            assert words_embedding_txt.is_file(), 'Words embedding not available'\n","            embeddings = self.convert_model_to_pickle()\n","        else:\n","            embeddings = self.load_model_from_pickle()\n","        self.embeddings = embeddings\n","        # build the vocabulary hashmap\n","        index = np.arange(self.embeddings.shape[0])\n","        # Dicctionarios para traducir de embedding a IDX de la palabra\n","        self.word2idx = dict(zip(self.embeddings['word'], index))\n","        self.idx2word = dict(zip(index, self.embeddings['word']))\n","\n","    def get_words_embeddings(self, words):\n","        words_idxs = self.words2idxs(words)\n","        return self.embeddings[words_idxs]['embedding']\n","\n","    def words2idxs(self, words):\n","        return np.array([self.word2idx.get(word, -1) for word in words])\n","\n","    def idxs2words(self, idxs):\n","        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n","\n","    def load_model_from_pickle(self):\n","        self.logger.debug(\n","            'loading words embeddings from pickle {}'.format(\n","                self.PKL_PATH\n","            )\n","        )\n","        max_bytes = 2**28 - 1 # 256MB\n","        bytes_in = bytearray(0)\n","        input_size = os.path.getsize(self.PKL_PATH)\n","        with open(self.PKL_PATH, 'rb') as f_in:\n","            for _ in range(0, input_size, max_bytes):\n","                bytes_in += f_in.read(max_bytes)\n","        embeddings = pickle.loads(bytes_in)\n","        self.logger.debug('words embeddings loaded')\n","        return embeddings\n","\n","    def convert_model_to_pickle(self):\n","        # create a numpy strctured array:\n","        # word     embedding\n","        # U50      np.float32[]\n","        # word_1   a, b, c\n","        # word_2   d, e, f\n","        # ...\n","        # word_n   g, h, i\n","        self.logger.debug(\n","            'converting and loading words embeddings from text file {}'.format(\n","                self.WORD_TO_VEC_MODEL_TXT_PATH\n","            )\n","        )\n","        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n","                     ('embedding', np.float32, (self.N_FEATURES,))]\n","        structure = np.dtype(structure)\n","        # load numpy array from disk using a generator\n","        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n","            embeddings_gen = (\n","                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n","                if len(line.split()[1:]) == self.N_FEATURES\n","            )\n","            embeddings = np.fromiter(embeddings_gen, structure)\n","        # add a null embedding\n","        null_embedding = np.array(\n","            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n","            dtype=structure\n","        )\n","        embeddings = np.concatenate([embeddings, null_embedding])\n","        # dump numpy array to disk using pickle\n","        max_bytes = 2**28 - 1 # # 256MB\n","        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n","        with open(self.PKL_PATH, 'wb') as f_out:\n","            for idx in range(0, len(bytes_out), max_bytes):\n","                f_out.write(bytes_out[idx:idx+max_bytes])\n","        self.logger.debug('words embeddings loaded')\n","        return embeddings\n","\n","\n","class GloveEmbeddings(WordsEmbeddings):\n","    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n","    PKL_PATH = 'gloveembedding.pkl'\n","    N_FEATURES = 50\n","    WORD_MAX_SIZE = 60\n","\n","\n","class FasttextEmbeddings(WordsEmbeddings):\n","    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n","    PKL_PATH = 'fasttext.pkl'\n","    N_FEATURES = 300\n","    WORD_MAX_SIZE = 60"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vo3STvSgp938"},"source":["model_fasttext = FasttextEmbeddings()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j2ECgkKCzSFt"},"source":["# Crear la Embedding matrix\n","\n","print('preparing embedding matrix...')\n","embed_dim = 300 # fasttext\n","words_not_found = []\n","\n","# word_index provieen del tokenizer\n","\n","nb_words = min(num_words, len(word_index)) # vocab_size\n","embedding_matrix = np.zeros((nb_words, embed_dim))\n","for word, i in word_index.items():\n","    if i >= nb_words:\n","        continue\n","    embedding_vector = model_fasttext.get_words_embeddings(word)[0]\n","    if (embedding_vector is not None) and len(embedding_vector) > 0:\n","        embedding_matrix[i] = embedding_vector\n","    else:\n","        # words not found in embedding index will be all-zeros.\n","        words_not_found.append(word)\n","        print(word)\n","\n","print('number of null word embeddings:', np.sum(np.sum(embedding_matrix, axis=1) == 0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"spx8pkTR28MN"},"source":["Embedding(input_dim=vocab_size,  # definido en el Tokenizador\n","          output_dim=embed_dim,  # dimensión de los embeddings utilizados\n","          input_length=in_shape, # máxima sentencia de entrada\n","          weights=[embedding_matrix],  # matrix de embeddings\n","          trainable=False))      # marcar como layer no entrenable"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N9tChByQrQmI"},"source":["model2 = Sequential()\n","# input_dim = vocab_size (max n_words)\n","# input_length = setencias con padding a 200\n","# output_dim = embed_dim (depende que embeddings pre entrenados utilizamos)\n","model2.add(Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=in_shape,\n","                     weights=[embedding_matrix], trainable=False))\n","\n","model2.add(LSTM(units=64, return_sequences=True))\n","model2.add(Dropout(0.2))\n","model2.add(LSTM(units=64)) # La última capa LSTM no lleva return_sequences\n","\n","model2.add(Dense(units=128, activation='relu'))\n","model2.add(Dropout(rate=0.2))\n","model2.add(Dense(units=out_shape, activation='sigmoid'))\n","\n","model2.compile(optimizer=\"adam\",\n","              loss='binary_crossentropy',\n","              metrics=['accuracy'])\n","\n","model2.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9s_Qx9xrzsE"},"source":["hist2 = model2.fit(X_train, y_train, epochs=5, validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qwfD04oXr5Sm"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","epoch_count = range(1, len(hist2.history['accuracy']) + 1)\n","sns.lineplot(x=epoch_count,  y=hist2.history['accuracy'], label='train')\n","sns.lineplot(x=epoch_count,  y=hist2.history['val_accuracy'], label='valid')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0OecxWJr95r"},"source":["model2.evaluate(X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3bjcXj8n32nO"},"source":["# Como este modelo es binario podemos calcular la curva ROC\n","curva_roc(model2, X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fGDesEf7sNrP"},"source":["### 4 - Conclusión\n","El modelo con embeddings pre-entrenados terminó con menos accuracy que el anterior, pero hay que tener en cuenta que este modelo no hizo overfitting y por lo tanto se lo puede seguir entrenando por más épocas o aumentar la complejidad del modelo."]}]}