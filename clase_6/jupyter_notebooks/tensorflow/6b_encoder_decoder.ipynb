{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6b_encoder_decoder.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pfa39F4lsLf3"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## LSTM encoder-decoder"]},{"cell_type":"markdown","metadata":{"id":"ZqO0PRcFsPTe"},"source":["### 1 - Datos\n","El objecto es utilizar una serie de sucuencias númericas (datos sintéticos) para poner a prueba el uso de las redes LSTM. Este ejemplo se inspiró en otro artículo, lo tienen como referencia en el siguiente link:\\\n","[LINK](https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras-part-2/)"]},{"cell_type":"code","source":["# Usar cuando TF migre a una version >= 2.9\n","# import re\n","\n","# import numpy as np\n","# import pandas as pd\n","\n","# import tensorflow as tf\n","# from keras.preprocessing.text import one_hot\n","# from tensorflow.keras.utils import pad_sequences\n","# from keras.models import Sequential\n","# from keras.layers.core import Activation, Dropout, Dense\n","# from keras.layers import Flatten, LSTM, SimpleRNN\n","# from keras.models import Model\n","# from tensorflow.keras.layers import Embedding\n","# from sklearn.model_selection import train_test_split\n","# from keras.preprocessing.text import Tokenizer\n","# from keras.layers import Input"],"metadata":{"id":"c4tr8mvoCvJD"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cq3YXak9sGHd"},"source":["import re\n","\n","import numpy as np\n","import pandas as pd\n","\n","from keras.preprocessing.text import one_hot\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers.core import Activation, Dropout, Dense\n","from keras.layers import Flatten, LSTM, SimpleRNN\n","from keras.models import Model\n","from keras.layers.embeddings import Embedding\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.layers import Input"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-9aNLZBDtA5J"},"source":["# Generar datos sintéticos\n","X = list()\n","y = list()\n","\n","# En ambos casos \"X\" e \"y\" son vectores de números de 5 en 5\n","X = [x for x in range(5, 301, 5)]\n","y = [x+15 for x in X]\n","\n","print(f\"datos X (len={len(X)}):\", X)\n","print(f\"datos y (len={len(y)}):\", y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ACkrI0GtEAM"},"source":["# Se desea agrupar los datos de a 3 elementos\n","X = np.array(X).reshape(len(X)//3, 3, 1)\n","y = np.array(y).reshape(len(y)//3, 3, 1)\n","print(\"datos X[0:2]:\", X[0:2])\n","print(\"datos y[0:2]:\", y[0:2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ajykLcJ8tFfN"},"source":["# Verificamos que la secuencia enetrada es igual a la secuencia de salida\n","# en cuanto a dimensiones\n","# Tendremos:\n","#  --> veinte grupos de datos (rows) (20)\n","#  --> cada grupo compuesto por tres elementos (3)\n","#  --> cada elemento representado en una sola dimension (1)\n","print(\"X shape:\", X.shape)\n","print(\"y shape:\", y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LZz9Zsvy5ilc"},"source":["### 2 - Preprocesamiento"]},{"cell_type":"code","source":["from sklearn import preprocessing\n","# Armar el \"courpus\" de números\n","data = np.append(X, y)\n","\n","# Identificar el vocabualario del corpus (todos los números únicos)\n","labels = list(np.unique(data))\n","\n","# Al vocabulario (labels) agregarle el token de inicio de secuencia\n","# Se utiliza el número \"0\" para el <sos>, aprovechando que el \"0\"\n","# no se encuentra en el vocabulario\n","labels = [0] + labels # <sos> + vocabulario\n","\n","label_encoder = preprocessing.LabelEncoder()\n","label_encoder.fit(labels)\n","label_encoder.classes_"],"metadata":{"id":"nCTu6NElI1ge"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J86H4qb6NEQp"},"source":["from keras.utils.np_utils import to_categorical\n","\n","# Preparar datos para consumir por la layers LSTM\n","# Por cada dato de X e Y se genera su contraparte oneHotEncoding\n","# X1 equivale a X, es la secuencia de entrada\n","# X2 equivale a y sin el ultimo elemento, equivale a la secuenca \"estado anterior\"\n","# target equivale a Y como OneHotEncoding, la prediccion completa con el ultimo elemento\n","\n","def get_dataset(X, Y, label_encoder):\n","\n","    cardinality = len(label_encoder.classes_)\n","    print(\"Number of features/cardinality:\", cardinality)\n","\n","    X1, X2, target = list(), list(), list()\n","    for x, y in zip(X, Y):\n","        input = list(label_encoder.transform(x.reshape(-1)))\n","        output = list(label_encoder.transform(y.reshape(-1)))\n","        # Crear la entrada del \"ultimo estado\" de salida\n","        # que es la salida sin el ultimo elemento, que es el que el modelo\n","        # debe predecir\n","        output_in = [0] + output[:-1]\n","\n","        # transformar\n","        input_encoded = to_categorical(input, num_classes=cardinality)\n","        output_encoded = to_categorical(output, num_classes=cardinality)\n","        output_in_encoded = to_categorical(output_in, num_classes=cardinality)\n","        \n","        # almacenar\n","        X1.append(input_encoded)\n","        X2.append(output_in_encoded)\n","        target.append(output_encoded)\n","    return np.array(X1), np.array(X2), np.array(target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pO2sS3ntkeEf"},"source":["X1, X2, target = get_dataset(X, y, label_encoder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El oneHotEncoding tiene una diemsión de -> vocabulario + tokens_especialiciales:\n","- 63 números únicos (vocab)\n","- 1 token especial (sos como id=0)"],"metadata":{"id":"Qu4pTKAm8UWQ"}},{"cell_type":"code","metadata":{"id":"XXEI1Rfkgngt"},"source":["# El vector de salida tiene 3 dimensiones:\n","# primera dimension es la cantidad de \"rows\" del dataset\n","# segunda dimension es el tamaño de la sequencia de entrada/salida\n","# tercera dimensión es la dimensionalidad del vector oneHotEncoding (cardinality)\n","\n","print(\"X1 shape:\", X1.shape)\n","print(\"X2 shape:\", X2.shape)\n","print(\"target shape:\", target.shape)\n","\n","in_features = X1.shape[-1]\n","output_features = target.shape[-1]\n","print(\"Number of input_features\", in_features)\n","print(\"Number of output_features\", output_features)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3vKbhjtIwPgM"},"source":["### 3 - Entrenar el modelo"]},{"cell_type":"code","metadata":{"id":"t_urD1qO2kOx"},"source":["# El código a continuación se puede utilizar para cualquier encoder-decoder\n","# que se desee construir con LSTM en el futuro\n","# y es posiblñe cambiar la layer LSTM por otra (CNN, Attention, etc)\n","# Fuente:\n","# https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n","# https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n","\n","from keras.models import Model\n","from keras.layers import Input, LSTM, Dense\n","\n","# returns train, inference_encoder and inference_decoder models\n","def define_models(n_input, n_output, n_units):\n","    '''\n","    Args:\n","        n_input: The cardinality of the input sequence, e.g. number of features, words, or characters for each time step.\n","        n_output: The cardinality of the output sequence, e.g. number of features, words, or characters for each time step.\n","        n_units: The number of cells to create in the encoder and decoder models, e.g. 128 or 256.\n","    output:\n","        train: Model that can be trained given source, target, and shifted target sequences.\n","        inference_encoder: Encoder model used when making a prediction for a new source sequence.\n","        inference_decoder Decoder model use when making a prediction for a new source sequence.\n","    '''\n","    # define training encoder\n","    encoder_inputs = Input(shape=(None, n_input))\n","    encoder = LSTM(n_units, return_state=True)\n","    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","    encoder_states = [state_h, state_c]\n","\n","    # define training decoder\n","    decoder_inputs = Input(shape=(None, n_output))\n","    decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n","    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","    decoder_dense = Dense(n_output, activation='softmax')\n","    decoder_outputs = decoder_dense(decoder_outputs)\n","    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","    # define inference encoder\n","    encoder_model = Model(encoder_inputs, encoder_states)\n","\n","    # define inference decoder\n","    decoder_state_input_h = Input(shape=(n_units,))\n","    decoder_state_input_c = Input(shape=(n_units,))\n","    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n","    decoder_states = [state_h, state_c]\n","    decoder_outputs = decoder_dense(decoder_outputs)\n","    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n","    # return all models\n","    return model, encoder_model, decoder_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uHtV5z_wGOwj"},"source":["model, encoder_model, decoder_model = define_models(in_features, output_features, n_units=128)\n","\n","model.compile(loss='mse', optimizer=\"Adam\")\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Conclusiones hasta ahora sobre el modelo:\n","- Nunca se especificó en la layer de entrada cuantos elementos recibirá el sistema, es por eso que el modelo se creó para aceptar una cantidad dinámica de batch y size de elementos de 64 dimensiones --> None, None, 64\n","- El primer None corresponde al batch dinámico\n","- El segundo None corresponde al la cantidad de elementos dinámico\n","\n","¿Cómo sabe cuantos elementos deberá esperar el sistema?\n","- En este caso siempre entregaremos 3 elementos al encoder y esperaremos 3 elementos del dencoder.\n","- En un siguiente ejemplo de seq2seq utilizaremos tokens especiales para ello."],"metadata":{"id":"v8PlXsXeuGCT"}},{"cell_type":"code","metadata":{"id":"2ljAyiBbG10U"},"source":["# Modelo completo (encoder+decoder) para poder entrenar\n","from keras.utils.vis_utils import plot_model\n","plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s1Wc1pnhIKJ6"},"source":["# Modelo solo encoder\n","from keras.utils.vis_utils import plot_model\n","plot_model(encoder_model, to_file='encoder_plot.png', show_shapes=True, show_layer_names=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_xanat4INez"},"source":["# Modelo solo decoder (para realizar inferencia)\n","from keras.utils.vis_utils import plot_model\n","plot_model(decoder_model, to_file='decoder_plot.png', show_shapes=True, show_layer_names=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VnlIx1Vezjwc"},"source":["hist = model.fit([X1, X2], target, epochs=150, validation_split=0.2, batch_size=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVz1uug_zu2J"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Entrenamiento\n","epoch_count = range(1, len(hist.history['loss']) + 1)\n","sns.lineplot(x=epoch_count,  y=hist.history['loss'], label='train')\n","sns.lineplot(x=epoch_count,  y=hist.history['val_loss'], label='valid')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"71XeCtfYmOFx"},"source":["def one_hot_decode(encoded_seq, label_encoder):\n","    idx = [np.argmax(vector) for vector in encoded_seq]\n","    return label_encoder.inverse_transform(idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lR1gKJN2m2Fw"},"source":["# Ensayo\n","x_test = [20, 25, 30]\n","y_test = [x+15 for x in x_test]\n","\n","print(\"y_test:\", y_test)\n","\n","# Transformar los datos a oneHotEncoding\n","X1_test, X2_test, target_test = get_dataset(np.array([x_test]), np.array([y_test]), label_encoder)\n","\n","print(\"X1_test shape:\", X1_test.shape)\n","print(\"X2_test shape:\", X2_test.shape)\n","print(\"target_test shape:\", target_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kluBZJGMm3LF"},"source":["# Cuando quiera por ejemplo recuperar target_test a y_test utilizo:\n","one_hot_decode(target_test[0], label_encoder)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C4_UQshIzw7o"},"source":["cardinality = len(label_encoder.classes_)\n","\n","# encode\n","# Se transforma la sequencia de entrada a los stados \"h\" y \"c\" de la LSTM\n","# para enviar la primera vez al decoder\n","state = encoder_model.predict(X1_test)\n","# start of sequence input --> la primera secuencia de salida-entrada (output_in)\n","# comienza con el ID cero (el ID seleccionado para <sos>)\n","output_in = to_categorical(0, num_classes=cardinality)\n","target_seq = np.array(output_in).reshape(1, 1, cardinality)\n","print(\"target_seq shape:\", target_seq.shape)\n","\n","# Vector de predicción\n","output = list()\n","\n","for t in range(3):\n","    # Predicción del próximo elemento\n","    print(\"Que recibe como elemlento anterior:\", one_hot_decode(target_seq[0], label_encoder))\n","    y_hat, h, c = decoder_model.predict([target_seq] + state)\n","    \n","    # Almacenar la predicción\n","    output.append(y_hat[0])\n","    print(\"Que saca como elemlento nuevo o prediccion:\", one_hot_decode(y_hat[0], label_encoder))\n","    # Actualizar los estados dado la ultimo prediccion\n","    state = [h, c]\n","    \n","    # Actualizar secuencia de entrada con la salida (re-alimentacion)\n","    target_seq = y_hat\n","\n","print(\"y_test:\", y_test)\n","print(\"y_hat:\", one_hot_decode(output, label_encoder))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M7MD_XS2oKUj"},"source":["# Podría si quisiera usar el \"evaluate\" de Keras pero ahí si necesitaré\n","# de X2:\n","model.evaluate([X1_test, X2_test], [target_test])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pkOjSJweqdF8"},"source":["### 4 - Conclusión\n","A primera vista parece muy compleja la estructura del encoder-decoder, pero funciona igual en cualquier disciplina de deeplearning:\n","- En visión para transferencia de estilo, generación de imagenes, etc.\n","- En NLP desde LSTM hasta Attention y transformes\n","\n","Hay que pensar el encoder como el generador del \"espacio latente\". Luego el decoder necesita el espacio latente que representa a la setencia de entrada, la realimentación de los valores de salida del decoder y los estados internos de la LSTM para pasar a la siguiente inferencia hasta concluir la secuencia de salida."]}]}