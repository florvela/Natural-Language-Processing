{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43564590",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "While bag of words and one-hot encoding are straightforward and easy to implement, they have limitations. The main drawbacks are:\n",
    "\n",
    "1) **High Dimensionality**: For a large vocabulary, the resulting vectors are very high-dimensional and sparse.\n",
    "\n",
    "2) **Lack of Semantic Information**: The techniques do not capture any semantic relationships between words. For instance, \"cat\" and \"dog\" are represented as entirely different vectors with no indication that they are semantically related.\n",
    "\n",
    "To address the limitations mentioned before, TF-IDF (Term Frequency-Inverse Document Frequency) emerged as a more efficient way to represent words. It adjusts word counts by their importance across documents, giving more weight to rare but informative words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2186183",
   "metadata": {},
   "source": [
    "TF-IDF combines two measures:\n",
    "\n",
    "1) Term Frequency (TF): The frequency of a term in a document.\n",
    "2) Inverse Document Frequency (IDF): A measure of how much information the word provides, based on its rarity across the entire corpus.\n",
    "\n",
    "If a term appears in all documents, the IDF will be zero (it is a popular term, and therefore it provides little value)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e094ca2c",
   "metadata": {},
   "source": [
    "![tf-idf-formula.jpg](../imgs/tf-idf-formula.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3610e338",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d41b3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4b9e24",
   "metadata": {},
   "source": [
    "Define the corpus with a few sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651b3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"I love cats.\",\n",
    "    \"I hate cats and dogs.\",\n",
    "    \"I have a dog.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Never odd or even.\",\n",
    "    \"Was it a car or a cat I saw?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daa8709",
   "metadata": {},
   "source": [
    "## Function to calculate TF-IDF manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "524251d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf_idf(corpus):\n",
    "    # Calculate Term Frequency (TF)\n",
    "    tf = []\n",
    "    for doc in corpus:\n",
    "        doc_tf = {}\n",
    "        words = doc.split()\n",
    "        for word in words:\n",
    "            doc_tf[word] = doc_tf.get(word, 0) + 1\n",
    "        doc_len = len(words)\n",
    "        for word in doc_tf:\n",
    "            doc_tf[word] /= doc_len\n",
    "        tf.append(doc_tf)\n",
    "    \n",
    "    # Calculate Document Frequency (DF)\n",
    "    df = {}\n",
    "    for doc in corpus:\n",
    "        words = set(doc.split())\n",
    "        for word in words:\n",
    "            df[word] = df.get(word, 0) + 1\n",
    "    \n",
    "    # Calculate Inverse Document Frequency (IDF)\n",
    "    N = len(corpus)\n",
    "    idf = {word: np.log(N / df[word]) for word in df}\n",
    "    \n",
    "    # Calculate TF-IDF\n",
    "    tf_idf = []\n",
    "    for doc_tf in tf:\n",
    "        doc_tf_idf = {word: doc_tf[word] * idf[word] for word in doc_tf}\n",
    "        tf_idf.append(doc_tf_idf)\n",
    "    \n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f78eb58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF-IDF manually\n",
    "manual_tfidf = calculate_tf_idf(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeabf899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manually Calculated TF-IDF:\n",
      "Document 1: {'que': 0.27465307216702745, 'dia': 0.1013662770270411, 'es': 0.1013662770270411, 'hoy': 0.1013662770270411}\n",
      "Document 2: {'martes': 0.11584717374518982, 'el': 0.15694461266687282, 'dia': 0.05792358687259491, 'de': 0.15694461266687282, 'hoy': 0.05792358687259491, 'es': 0.05792358687259491}\n",
      "Document 3: {'martes': 0.13515503603605478, 'muchas': 0.3662040962227032, 'gracias': 0.3662040962227032}\n"
     ]
    }
   ],
   "source": [
    "# Display the manually calculated TF-IDF\n",
    "print(\"\\nManually Calculated TF-IDF:\")\n",
    "for idx, doc_tf_idf in enumerate(manual_tfidf):\n",
    "    print(f\"Document {idx + 1}: {doc_tf_idf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7617db",
   "metadata": {},
   "source": [
    "## TF-IDF with Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c80cc",
   "metadata": {},
   "source": [
    "By default, `TfidfVectorizer` in sklearn uses sublinear TF scaling, where the term frequency is calculated as $$ 1 + \\log(\\text{TF}) $$\n",
    "\n",
    "Additionally, `TfidfVectorizer` computes the inverse document frequency (IDF) using the formula $$\\text{IDF}(t) = \\log\\left(\\frac{1 + N}{1 + \\text{df}(t)}\\right) + 1 $$\n",
    "\n",
    "where \\(N\\) is the total number of documents and $$\\text{df}(t)$$ is the number of documents containing the term t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfa878c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "257f3f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18cd6b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TF-IDF matrix to a DataFrame for better readability\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fc1c9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Matrix:\n",
      "         de       dia        el        es   gracias       hoy    martes  \\\n",
      "0  0.000000  0.459854  0.000000  0.459854  0.000000  0.459854  0.000000   \n",
      "1  0.406598  0.309228  0.406598  0.309228  0.000000  0.309228  0.618457   \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.622766  0.000000  0.473630   \n",
      "\n",
      "     muchas       que  \n",
      "0  0.000000  0.604652  \n",
      "1  0.000000  0.000000  \n",
      "2  0.622766  0.000000  \n"
     ]
    }
   ],
   "source": [
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a01aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
