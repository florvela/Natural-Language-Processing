{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue5hxxkdAQJg"
      },
      "source": [
        "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
        "\n",
        "\n",
        "# Procesamiento de lenguaje natural\n",
        "## Preprocesamiento con NLTK y Spacy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCED1hh-Ioyf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import string\n",
        "import random \n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMOa4JPSCJ29"
      },
      "source": [
        "### Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdW55pJin1rt"
      },
      "outputs": [],
      "source": [
        "simple_text = \"if she leaves now she might miss something important!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIO7b8GjAC17"
      },
      "outputs": [],
      "source": [
        "large_text = \"Patients who in late middle age have smoked 20 cigarettes a day since their teens constitute an at-risk group. One thing they’re clearly at risk for is the acute sense of guilt that a clinician can incite, which immediately makes a consultation tense.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVHxBRNzCMOS"
      },
      "source": [
        "### 1 - Preprocesamiento con NLTK\n",
        "- Cada documento transformarlo en una lista de términos\n",
        "- Armar un vector de términos no repetidos de todos los documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM-lmmsFnC6X"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize  \n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Descargar el diccionario\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soL5T-UDsZ20"
      },
      "outputs": [],
      "source": [
        "simple_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Er9fvFonfT1"
      },
      "outputs": [],
      "source": [
        "# Crear el derivador\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "p_stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuEob1D6nEPK"
      },
      "outputs": [],
      "source": [
        "# Crear el lematizador\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE9pq3dMod6Y"
      },
      "outputs": [],
      "source": [
        "# Extraer los tokens de un doc\n",
        "tokens = word_tokenize(simple_text)\n",
        "print(\"Tokens:\", tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSdedQvVM-wN"
      },
      "outputs": [],
      "source": [
        "# Transformar los tokens a sus respectivas palabras derivadas\n",
        "# Stemming\n",
        "nltk_stemedList = []\n",
        "for word in tokens:\n",
        "    nltk_stemedList.append(p_stemmer.stem(word))\n",
        "print(\"Stemming:\", nltk_stemedList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OV3-wVBSNNaA"
      },
      "outputs": [],
      "source": [
        "# Transformar los tokens a sus respectivas palabras raiz\n",
        "# Lemmatization\n",
        "nltk_lemmaList = []\n",
        "for word in tokens:\n",
        "    nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
        "print(\"Lemmatization:\", nltk_lemmaList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFzuIwPZuNqo"
      },
      "outputs": [],
      "source": [
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U47nxm8ZNiIr"
      },
      "outputs": [],
      "source": [
        "# Quitar los signos de puntuacion\n",
        "nltk_punctuation = [w for w in nltk_lemmaList if w not in string.punctuation]\n",
        "print(\"Punctuation filter:\", nltk_punctuation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A71y7Ecsun-u"
      },
      "outputs": [],
      "source": [
        "nltk_stop_words = set(stopwords.words(\"english\"))\n",
        "len(nltk_stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImlO-N45OuKG"
      },
      "outputs": [],
      "source": [
        "# Stop words\n",
        "nltk_stop_words = set(stopwords.words(\"english\"))\n",
        "filtered_sentence = [w for w in nltk_punctuation if w not in nltk_stop_words]\n",
        "print(\"Stop words filter:\", filtered_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NrPtt2OmWBv"
      },
      "source": [
        "### 2 - Proceso completo con NLTK\n",
        "Tokenization → Lemmatization → Remove stopwords → Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZqTOZzDI7uv"
      },
      "outputs": [],
      "source": [
        "def nltk_process(text):\n",
        "    # Tokenization\n",
        "    nltk_tokenList = word_tokenize(text)\n",
        "      \n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    nltk_lemmaList = []\n",
        "    for word in nltk_tokenList:\n",
        "        nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
        "    \n",
        "    print(\"Lemmatization\")\n",
        "    print(nltk_lemmaList)\n",
        "\n",
        "    # Stop words\n",
        "    nltk_stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_sentence = [w for w in nltk_lemmaList if w not in nltk_stop_words]\n",
        "\n",
        "    # Filter Punctuation\n",
        "    filtered_sentence = [w for w in filtered_sentence if w not in string.punctuation]\n",
        "\n",
        "    print(\" \")\n",
        "    print(\"Remove stopword & Punctuation\")\n",
        "    print(filtered_sentence)\n",
        "    return filtered_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZdiop6IJpZN"
      },
      "outputs": [],
      "source": [
        "nltk_text = nltk_process(large_text)\n",
        "print(\"Text len:\", len(nltk_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M4F0ll1msUY"
      },
      "source": [
        "### 3 - Proceso completo con spaCy\n",
        "Tokenization → Lemmatization → Remove stopwords → Remove punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r57e9b9Omwnh"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def spacy_process(text):\n",
        "    doc = nlp(text)\n",
        "    \n",
        "    # Tokenization & lemmatization\n",
        "    lemma_list = []\n",
        "    for token in doc:\n",
        "        lemma_list.append(token.lemma_)\n",
        "    print(\"Tokenize+Lemmatize:\")\n",
        "    print(lemma_list)\n",
        "    \n",
        "    # Stop words\n",
        "    filtered_sentence =[]\n",
        "    for word in lemma_list:\n",
        "        lexeme = nlp.vocab[word]\n",
        "        if lexeme.is_stop == False:\n",
        "            filtered_sentence.append(word) \n",
        "    \n",
        "    # Filter punctuation\n",
        "    filtered_sentence = [w for w in filtered_sentence if w not in string.punctuation]\n",
        "\n",
        "    print(\" \")\n",
        "    print(\"Remove stopword & punctuation: \")\n",
        "    print(filtered_sentence)\n",
        "    return filtered_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9x_iKHu1pKBE"
      },
      "outputs": [],
      "source": [
        "spacy_text = spacy_process(large_text)\n",
        "print(\"Text len:\", len(nltk_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txMZ7lR-pvHB"
      },
      "source": [
        "### 4 - Conclusiones\n",
        "- NLTK no pasa a minúsculas el texto por su cuenta\n",
        "- spacy algunas palabras las reemplaza por su Tag (como \"'\")\n",
        "- spacy descompone palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te3GgSNzpbGq"
      },
      "outputs": [],
      "source": [
        "from prettytable import PrettyTable\n",
        "table = PrettyTable(['NLTK', 'spaCy'])\n",
        "for nltk_word, spacy_word in zip(nltk_text, spacy_text):\n",
        "    table.add_row([nltk_word, spacy_word])\n",
        "print(table)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "2a - preprocesamiento.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}