{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue5hxxkdAQJg"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Preprocesamiento con NLTK y Spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kCED1hh-Ioyf"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import random \n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMOa4JPSCJ29"
   },
   "source": [
    "### Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gdW55pJin1rt"
   },
   "outputs": [],
   "source": [
    "simple_text = \"if she leaves now she might miss something important!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RIO7b8GjAC17"
   },
   "outputs": [],
   "source": [
    "large_text = \"Patients who in late middle age have smoked 20 cigarettes a day since their teens constitute an at-risk group. One thing they’re clearly at risk for is the acute sense of guilt that a clinician can incite, which immediately makes a consultation tense.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVHxBRNzCMOS"
   },
   "source": [
    "### 1 - Preprocesamiento con NLTK\n",
    "- Cada documento transformarlo en una lista de términos\n",
    "- Armar un vector de términos no repetidos de todos los documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wM-lmmsFnC6X"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/flor/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/flor/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /Users/flor/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/flor/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar tokenizador punkt\n",
    "nltk.download(\"punkt\")\n",
    "# Descargar diccionario de inglés\n",
    "nltk.download(\"wordnet\")\n",
    "# Descargar diccionario de stopwords\n",
    "nltk.download('stopwords')\n",
    "# Para usar NLTK 3.6.6 o superior es necesario instalar OMW 1.4 \n",
    "# (Open Multilingual WordNet)\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "soL5T-UDsZ20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if she leaves now she might miss something important!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1Er9fvFonfT1"
   },
   "outputs": [],
   "source": [
    "# Crear el derivador\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BuEob1D6nEPK"
   },
   "outputs": [],
   "source": [
    "# Crear el lematizador\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GE9pq3dMod6Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['if', 'she', 'leaves', 'now', 'she', 'might', 'miss', 'something', 'important', '!']\n"
     ]
    }
   ],
   "source": [
    "# Extraer los tokens de un doc\n",
    "tokens = word_tokenize(simple_text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lSdedQvVM-wN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming: ['if', 'she', 'leav', 'now', 'she', 'might', 'miss', 'someth', 'import', '!']\n"
     ]
    }
   ],
   "source": [
    "# Transformar los tokens a sus respectivas palabras derivadas\n",
    "# Stemming\n",
    "nltk_stemedList = []\n",
    "for word in tokens:\n",
    "    nltk_stemedList.append(p_stemmer.stem(word))\n",
    "print(\"Stemming:\", nltk_stemedList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OV3-wVBSNNaA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization: ['if', 'she', 'leaf', 'now', 'she', 'might', 'miss', 'something', 'important', '!']\n"
     ]
    }
   ],
   "source": [
    "# Transformar los tokens a sus respectivas palabras raiz\n",
    "# Lemmatization\n",
    "nltk_lemmaList = []\n",
    "for word in tokens:\n",
    "    nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
    "print(\"Lemmatization:\", nltk_lemmaList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jFzuIwPZuNqo"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "U47nxm8ZNiIr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation filter: ['if', 'she', 'leaf', 'now', 'she', 'might', 'miss', 'something', 'important']\n"
     ]
    }
   ],
   "source": [
    "# Quitar los signos de puntuacion\n",
    "nltk_punctuation = [w for w in nltk_lemmaList if w not in string.punctuation]\n",
    "print(\"Punctuation filter:\", nltk_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "A71y7Ecsun-u"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "len(nltk_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ImlO-N45OuKG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words filter: ['leaf', 'might', 'miss', 'something', 'important']\n"
     ]
    }
   ],
   "source": [
    "# Stop words\n",
    "nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_sentence = [w for w in nltk_punctuation if w not in nltk_stop_words]\n",
    "print(\"Stop words filter:\", filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NrPtt2OmWBv"
   },
   "source": [
    "### 2 - Proceso completo con NLTK\n",
    "Tokenization → Lemmatization → Remove stopwords → Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3ZqTOZzDI7uv"
   },
   "outputs": [],
   "source": [
    "def nltk_process(text):\n",
    "    # Tokenization\n",
    "    nltk_tokenList = word_tokenize(text)\n",
    "      \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    nltk_lemmaList = []\n",
    "    for word in nltk_tokenList:\n",
    "        nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    print(\"Lemmatization\")\n",
    "    print(nltk_lemmaList)\n",
    "\n",
    "    # Stop words\n",
    "    nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_sentence = [w for w in nltk_lemmaList if w not in nltk_stop_words]\n",
    "\n",
    "    # Filter Punctuation\n",
    "    filtered_sentence = [w for w in filtered_sentence if w not in string.punctuation]\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"Remove stopword & Punctuation\")\n",
    "    print(filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "CZdiop6IJpZN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization\n",
      "['Patients', 'who', 'in', 'late', 'middle', 'age', 'have', 'smoked', '20', 'cigarette', 'a', 'day', 'since', 'their', 'teen', 'constitute', 'an', 'at-risk', 'group', '.', 'One', 'thing', 'they', '’', 're', 'clearly', 'at', 'risk', 'for', 'is', 'the', 'acute', 'sense', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incite', ',', 'which', 'immediately', 'make', 'a', 'consultation', 'tense', '.']\n",
      " \n",
      "Remove stopword & Punctuation\n",
      "['Patients', 'late', 'middle', 'age', 'smoked', '20', 'cigarette', 'day', 'since', 'teen', 'constitute', 'at-risk', 'group', 'One', 'thing', '’', 'clearly', 'risk', 'acute', 'sense', 'guilt', 'clinician', 'incite', 'immediately', 'make', 'consultation', 'tense']\n",
      "Text len: 27\n"
     ]
    }
   ],
   "source": [
    "nltk_text = nltk_process(large_text)\n",
    "print(\"Text len:\", len(nltk_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_M4F0ll1msUY"
   },
   "source": [
    "### 3 - Proceso completo con spaCy\n",
    "Tokenization → Lemmatization → Remove stopwords → Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (3.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: setuptools in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (8.1.0)\n",
      "Requirement already satisfied: jinja2 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2017.7.27.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: jinja2 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.59.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.20.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.27.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: setuptools in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (20.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.7.4.3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2017.7.27.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install spacy\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "r57e9b9Omwnh"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Cargar pipeline de preprocesamiento de inglés\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_process(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenization & lemmatization\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "    print(\"Tokenize+Lemmatize:\")\n",
    "    print(lemma_list)\n",
    "    \n",
    "    # Stop words\n",
    "    filtered_sentence =[]\n",
    "    for word in lemma_list:\n",
    "        # word es un string, para recuperar la información de los objetos de SpaCy\n",
    "        # necesitamos usar el string para pasar a un lexema, el objeto de SpaCy\n",
    "        # que para cada término contiene la información del preprocesamiento\n",
    "        # (se podría también directamente filtrar stopwords en el paso de lematización)\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word) \n",
    "    \n",
    "    # Filter punctuation\n",
    "    filtered_sentence = [w for w in filtered_sentence if w not in string.punctuation]\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"Remove stopword & punctuation: \")\n",
    "    print(filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "9x_iKHu1pKBE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize+Lemmatize:\n",
      "['patient', 'who', 'in', 'late', 'middle', 'age', 'have', 'smoke', '20', 'cigarette', 'a', 'day', 'since', 'their', 'teen', 'constitute', 'an', 'at', '-', 'risk', 'group', '.', 'one', 'thing', 'they', '’re', 'clearly', 'at', 'risk', 'for', 'be', 'the', 'acute', 'sense', 'of', 'guilt', 'that', 'a', 'clinician', 'can', 'incite', ',', 'which', 'immediately', 'make', 'a', 'consultation', 'tense', '.']\n",
      " \n",
      "Remove stopword & punctuation: \n",
      "['patient', 'late', 'middle', 'age', 'smoke', '20', 'cigarette', 'day', 'teen', 'constitute', 'risk', 'group', 'thing', 'clearly', 'risk', 'acute', 'sense', 'guilt', 'clinician', 'incite', 'immediately', 'consultation', 'tense']\n",
      "Text len: 27\n"
     ]
    }
   ],
   "source": [
    "spacy_text = spacy_process(large_text)\n",
    "print(\"Text len:\", len(nltk_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txMZ7lR-pvHB"
   },
   "source": [
    "### 4 - Conclusiones\n",
    "- NLTK no pasa a minúsculas el texto por su cuenta\n",
    "- spacy algunas palabras las reemplaza por su Tag (como \"'\")\n",
    "- spacy descompone palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytable\n",
      "  Downloading prettytable-3.4.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: wcwidth in /Users/flor/opt/anaconda3/lib/python3.8/site-packages (from prettytable) (0.2.5)\n",
      "Installing collected packages: prettytable\n",
      "Successfully installed prettytable-3.4.1\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Te3GgSNzpbGq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|    NLTK    |    spaCy     |\n",
      "+------------+--------------+\n",
      "|  Patients  |   patient    |\n",
      "|    late    |     late     |\n",
      "|   middle   |    middle    |\n",
      "|    age     |     age      |\n",
      "|   smoked   |    smoke     |\n",
      "|     20     |      20      |\n",
      "| cigarette  |  cigarette   |\n",
      "|    day     |     day      |\n",
      "|   since    |     teen     |\n",
      "|    teen    |  constitute  |\n",
      "| constitute |     risk     |\n",
      "|  at-risk   |    group     |\n",
      "|   group    |    thing     |\n",
      "|    One     |   clearly    |\n",
      "|   thing    |     risk     |\n",
      "|     ’      |    acute     |\n",
      "|  clearly   |    sense     |\n",
      "|    risk    |    guilt     |\n",
      "|   acute    |  clinician   |\n",
      "|   sense    |    incite    |\n",
      "|   guilt    | immediately  |\n",
      "| clinician  | consultation |\n",
      "|   incite   |    tense     |\n",
      "+------------+--------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "table = PrettyTable(['NLTK', 'spaCy'])\n",
    "for nltk_word, spacy_word in zip(nltk_text, spacy_text):\n",
    "    table.add_row([nltk_word, spacy_word])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2a - preprocesamiento.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
