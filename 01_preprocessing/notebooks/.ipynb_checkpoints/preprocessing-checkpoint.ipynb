{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue5hxxkdAQJg"
   },
   "source": [
    "# Natural language processing\n",
    "## Preprocessing with NLTK and Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kCED1hh-Ioyf"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import json\n",
    "import string\n",
    "import random \n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVHxBRNzCMOS"
   },
   "source": [
    "### 1 - Preprocessing with NLTK\n",
    "- Transform each document into a list of terms\n",
    "- Create a vector of non-repeated terms from all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wM-lmmsFnC6X"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/florenciavela/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/florenciavela/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/florenciavela/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/florenciavela/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# For using NLTK 3.6.6, you need to install OMW 1.4 \n",
    "# (Open Multilingual WordNet)\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "soL5T-UDsZ20"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text preprocessing is a crucial step in any Natural Language Processing (NLP) project. Before gathering the features, we need to preprocess the text to ensure it is in a clean and structured format!!!1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample text for preprocessing\n",
    "text = \"Text preprocessing is a crucial step in any Natural Language Processing (NLP) project. Before gathering the features, we need to preprocess the text to ensure it is in a clean and structured format!!!1\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization\n",
    "\n",
    "Tokenization is the process of splitting a sentence or document into individual words or terms. This is a fundamental step, as it simplifies the text into manageable pieces. For example, the sentence “NLP is fascinating” would be tokenized into [“NLP”, “is”, “fascinating”].\n",
    "\n",
    "![Tokenization](../imgs/tokenization.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Text', 'preprocessing', 'is', 'a', 'crucial', 'step', 'in', 'any', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'project', '.', 'Before', 'gathering', 'the', 'features', ',', 'we', 'need', 'to', 'preprocess', 'the', 'text', 'to', 'ensure', 'it', 'is', 'in', 'a', 'clean', 'and', 'structured', 'format', '!', '!', '!', '1']\n"
     ]
    }
   ],
   "source": [
    "# Splitting the text into individual words\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing puntuation\n",
    "\n",
    "Punctuation marks are usually removed from the text since they often do not carry significant meaning for text analysis. This helps in reducing the complexity of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without punctuation: ['Text', 'preprocessing', 'is', 'a', 'crucial', 'step', 'in', 'any', 'Natural', 'Language', 'Processing', 'NLP', 'project', 'Before', 'gathering', 'the', 'features', 'we', 'need', 'to', 'preprocess', 'the', 'text', 'to', 'ensure', 'it', 'is', 'in', 'a', 'clean', 'and', 'structured', 'format', '1']\n"
     ]
    }
   ],
   "source": [
    "# Removing punctuation marks from tokens\n",
    "tokens_no_punct = [word for word in tokens if word.isalnum()]\n",
    "print(\"Tokens without punctuation:\", tokens_no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing numbers\n",
    "\n",
    "Numbers are also removed unless they carry significant meaning for the specific NLP task. This is because numbers can add noise to the data, affecting the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without numbers: ['Text', 'preprocessing', 'is', 'a', 'crucial', 'step', 'in', 'any', 'Natural', 'Language', 'Processing', 'NLP', 'project', 'Before', 'gathering', 'the', 'features', 'we', 'need', 'to', 'preprocess', 'the', 'text', 'to', 'ensure', 'it', 'is', 'in', 'a', 'clean', 'and', 'structured', 'format']\n"
     ]
    }
   ],
   "source": [
    "# Removing numerical values from tokens\n",
    "tokens_no_numbers = [word for word in tokens_no_punct if not word.isdigit()]\n",
    "print(\"Tokens without numbers:\", tokens_no_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing HTML or special characters\n",
    "\n",
    "In many cases, text data contains HTML tags or special symbols (e.g., &amp;, @, #). Removing these elements helps in cleaning the text, making it more suitable for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without special characters: ['Text', 'preprocessing', 'is', 'a', 'crucial', 'step', 'in', 'any', 'Natural', 'Language', 'Processing', 'NLP', 'project', 'Before', 'gathering', 'the', 'features', 'we', 'need', 'to', 'preprocess', 'the', 'text', 'to', 'ensure', 'it', 'is', 'in', 'a', 'clean', 'and', 'structured', 'format']\n"
     ]
    }
   ],
   "source": [
    "# Using regex to remove special characters\n",
    "tokens_cleaned = [re.sub(r'\\W+', '', word) for word in tokens_no_numbers]\n",
    "print(\"Tokens without special characters:\", tokens_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in lowercase: ['text', 'preprocessing', 'is', 'a', 'crucial', 'step', 'in', 'any', 'natural', 'language', 'processing', 'nlp', 'project', 'before', 'gathering', 'the', 'features', 'we', 'need', 'to', 'preprocess', 'the', 'text', 'to', 'ensure', 'it', 'is', 'in', 'a', 'clean', 'and', 'structured', 'format']\n"
     ]
    }
   ],
   "source": [
    "# Converting all tokens to lowercase to maintain uniformity\n",
    "tokens_lower = [word.lower() for word in tokens_cleaned]\n",
    "print(\"Tokens in lowercase:\", tokens_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words\n",
    "\n",
    "Stop words are common words (e.g., “and”, “the”, “is”) that do not contribute much to the meaning of a sentence. Removing these words can improve the efficiency of the model by reducing the volume of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without stop words: ['text', 'preprocessing', 'crucial', 'step', 'natural', 'language', 'processing', 'nlp', 'project', 'gathering', 'features', 'need', 'preprocess', 'text', 'ensure', 'clean', 'structured', 'format']\n"
     ]
    }
   ],
   "source": [
    "# Removing common words that do not contribute much to the meaning\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_no_stopwords = [word for word in tokens_lower if word not in stop_words]\n",
    "print(\"Tokens without stop words:\", tokens_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Stemming involves reducing words to their root form, which may not always be a dictionary word. For example, “running” and “runner” might both be reduced to “run”. Although this approach is simple, it can sometimes produce non-existent words.\n",
    "\n",
    "Examples:\n",
    "* “running” -> “run”\n",
    "* “runners” -> “runner”\n",
    "* “studies” -> “studi”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['text', 'preprocess', 'crucial', 'step', 'natur', 'languag', 'process', 'nlp', 'project', 'gather', 'featur', 'need', 'preprocess', 'text', 'ensur', 'clean', 'structur', 'format']\n"
     ]
    }
   ],
   "source": [
    "# Reducing words to their root form using PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens_no_stopwords]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization\n",
    "\n",
    "Lemmatization reduces words to their base or dictionary form. For instance, “running” and “ran” would be reduced to “run”. Unlike stemming, lemmatization ensures that the root word belongs to the language dictionary, maintaining semantic meaning.\n",
    "\n",
    "Examples:\n",
    "* “running” -> “run”\n",
    "* “runners” -> “runner”\n",
    "* “studies” -> “study”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['text', 'preprocessing', 'crucial', 'step', 'natural', 'language', 'processing', 'nlp', 'project', 'gathering', 'feature', 'need', 'preprocess', 'text', 'ensure', 'clean', 'structured', 'format']\n"
     ]
    }
   ],
   "source": [
    "# Reducing words to their dictionary form using WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_no_stopwords]\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stemming vs Lemmatization](../imgs/stemming-lemmatization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTLK preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_preprocess(text):\n",
    "    print(\"Initial text:\", text)\n",
    "    # Tokenization\n",
    "    nltk_tokenList = word_tokenize(text)\n",
    "      \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    nltk_lemmaList = []\n",
    "    for word in nltk_tokenList:\n",
    "        nltk_lemmaList.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    print(\"\\nLemmatization:\", nltk_lemmaList)\n",
    "\n",
    "    # Stop words\n",
    "    nltk_stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_sentence = [w for w in nltk_lemmaList if w not in nltk_stop_words]\n",
    "\n",
    "    # Filter Punctuation\n",
    "    filtered_sentence = [w for w in filtered_sentence if w not in string.punctuation]\n",
    "    \n",
    "    print(\"\\nRemove stopword & Punctuation: \", filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial text: Text preprocessing is a crucial step in any Natural Language Processing (NLP) project. Before gathering the features, we need to preprocess the text to ensure it is in a clean and structured format!!!1\n",
      "\n",
      "Lemmatization: ['Text', 'preprocessing', 'is', 'a', 'crucial', 'step', 'in', 'any', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'project', '.', 'Before', 'gathering', 'the', 'feature', ',', 'we', 'need', 'to', 'preprocess', 'the', 'text', 'to', 'ensure', 'it', 'is', 'in', 'a', 'clean', 'and', 'structured', 'format', '!', '!', '!', '1']\n",
      "\n",
      "Remove stopword & Punctuation:  ['Text', 'preprocessing', 'crucial', 'step', 'Natural', 'Language', 'Processing', 'NLP', 'project', 'Before', 'gathering', 'feature', 'need', 'preprocess', 'text', 'ensure', 'clean', 'structured', 'format', '1']\n"
     ]
    }
   ],
   "source": [
    "nltk_text = nltk_preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install spacy\n",
    "#!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Cargar pipeline de preprocesamiento de inglés\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def spacy_preprocess(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenization & lemmatization\n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        lemma_list.append(token.lemma_)\n",
    "    print(\"Tokenize+Lemmatize:\")\n",
    "    print(lemma_list)\n",
    "    \n",
    "    # Stop words\n",
    "    filtered_sentence =[]\n",
    "    for word in lemma_list:\n",
    "        # 'word' is a string. To retrieve information from spaCy objects,\n",
    "        # we need to use the string to get a lexeme, the spaCy object\n",
    "        # that contains preprocessing information for each term\n",
    "        # (we could also directly filter stopwords during the lemmatization step)\n",
    "        lexeme = nlp.vocab[word]\n",
    "        if lexeme.is_stop == False:\n",
    "            filtered_sentence.append(word) \n",
    "    \n",
    "    # Filter punctuation\n",
    "    filtered_sentence = [w for w in filtered_sentence if w not in string.punctuation]\n",
    "\n",
    "    print(\" \")\n",
    "    print(\"Remove stopword & punctuation: \")\n",
    "    print(filtered_sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "9x_iKHu1pKBE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize+Lemmatize:\n",
      "['text', 'preprocessing', 'be', 'a', 'crucial', 'step', 'in', 'any', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'project', '.', 'before', 'gather', 'the', 'feature', ',', 'we', 'need', 'to', 'preprocess', 'the', 'text', 'to', 'ensure', 'it', 'be', 'in', 'a', 'clean', 'and', 'structured', 'format!!!1']\n",
      " \n",
      "Remove stopword & punctuation: \n",
      "['text', 'preprocessing', 'crucial', 'step', 'Natural', 'Language', 'Processing', 'NLP', 'project', 'gather', 'feature', 'need', 'preprocess', 'text', 'ensure', 'clean', 'structured', 'format!!!1']\n"
     ]
    }
   ],
   "source": [
    "spacy_text = spacy_preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txMZ7lR-pvHB"
   },
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing is an essential step in NLP that transforms raw text into a format that can be effectively analyzed and modeled.\n",
    "By using techniques such as tokenization, lemmatization, and stop words removal, we can clean and structure the text data. Libraries like NLTK and spaCy provide robust tools for implementing these preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Te3GgSNzpbGq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+\n",
      "|      NLTK     |     spaCy     |\n",
      "+---------------+---------------+\n",
      "|      Text     |      text     |\n",
      "| preprocessing | preprocessing |\n",
      "|    crucial    |    crucial    |\n",
      "|      step     |      step     |\n",
      "|    Natural    |    Natural    |\n",
      "|    Language   |    Language   |\n",
      "|   Processing  |   Processing  |\n",
      "|      NLP      |      NLP      |\n",
      "|    project    |    project    |\n",
      "|     Before    |     gather    |\n",
      "|   gathering   |    feature    |\n",
      "|    feature    |      need     |\n",
      "|      need     |   preprocess  |\n",
      "|   preprocess  |      text     |\n",
      "|      text     |     ensure    |\n",
      "|     ensure    |     clean     |\n",
      "|     clean     |   structured  |\n",
      "|   structured  |   format!!!1  |\n",
      "+---------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "#!pip3 install prettytable\n",
    "from prettytable import PrettyTable\n",
    "table = PrettyTable(['NLTK', 'spaCy'])\n",
    "for nltk_word, spacy_word in zip(nltk_text, spacy_text):\n",
    "    table.add_row([nltk_word, spacy_word])\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2a - preprocesamiento.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
